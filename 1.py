

import requests
import feedparser
import os
import argparse
import time


class ArxivSearcher:
    def __init__(self, save_dir="D:\\PyCharm Community Edition 2024.2.1\\pythonProject\\downloads"):
        self.base_url = "http://export.arxiv.org/api/query"
        self.save_dir = save_dir
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            print(f"[+] åˆ›å»ºä¸‹è½½ç›®å½•: {save_dir}")

    def search(self, query, max_results=5):
        url = (
            f"{self.base_url}?search_query={query}"
            f"&start=0&max_results={max_results}"
            f"&sortBy=submittedDate&sortOrder=descending"
        )

        print(f"[+] æœç´¢æŸ¥è¯¢: {query}")
        print(f"[+] è¯·æ±‚URL: {url}")

        try:
            response = requests.get(url)
            response.raise_for_status()
            print("[âˆš] APIè¯·æ±‚æˆåŠŸ")

            feed = feedparser.parse(response.text)
            print(f"[+] æ‰¾åˆ° {len(feed.entries)} ç¯‡è®ºæ–‡")

            results = []

            for entry in feed.entries:
                pdf_link = ""
                for link in entry.links:
                    if link.type == "application/pdf":
                        pdf_link = link.href
                        break

                arxiv_id = entry.id.split('/')[-1]

                results.append({
                    "title": entry.title,
                    "authors": [author.name for author in entry.authors],
                    "summary": entry.summary,
                    "pdf_url": pdf_link,
                    "arxiv_id": arxiv_id,
                    "published": entry.published if hasattr(entry, 'published') else "Unknown"
                })

            return results

        except Exception as e:
            print(f"[!] æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return []

    def download_pdf(self, pdf_url, filename=None):

        if not filename:
            name = pdf_url.split("/")[-1]
            if not name.endswith(".pdf"):
                name += ".pdf"
            filename = name

        save_path = os.path.join(self.save_dir, filename)

        print(f"[+] å¼€å§‹ä¸‹è½½: {pdf_url}")
        print(f"[+] ä¿å­˜è·¯å¾„: {save_path}")

        try:
            response = requests.get(pdf_url, stream=True)
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))

            with open(save_path, "wb") as f:
                if total_size == 0:
                    f.write(response.content)
                else:
                    downloaded = 0
                    for data in response.iter_content(chunk_size=8192):
                        downloaded += len(data)
                        f.write(data)
            return save_path

        except Exception as e:
            return None


def download_deep_learning_paper():

    download_dir = r"D:\PyCharm Community Edition 2024.2.1\pythonProject\deep_learning_papers"

    searcher = ArxivSearcher(save_dir=download_dir)

    print("=" * 60)
    print("ArXiv æ·±åº¦å­¦ä¹ è®ºæ–‡æœç´¢ä¸ä¸‹è½½å·¥å…·")
    print("=" * 60)


    search_query = "deep learning"
    max_results = 3


    results = searcher.search(search_query, max_results=max_results)

    if not results:
        return

    # æ˜¾ç¤ºæœç´¢ç»“æœ
    print(f"\n[+] æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³è®ºæ–‡:")
    print("-" * 60)

    for i, paper in enumerate(results, 1):
        print(f"\nğŸ“„ è®ºæ–‡ #{i}:")
        print(f"   æ ‡é¢˜: {paper['title']}")
        print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
        print(f"   å‘å¸ƒæ—¶é—´: {paper['published']}")
        print(f"   arXiv ID: {paper['arxiv_id']}")
        print(f"   æ‘˜è¦é¢„è§ˆ: {paper['summary'][:200]}...")
        print("-" * 60)

    # ä¸‹è½½ç¬¬ä¸€ç¯‡è®ºæ–‡
    if results:
        first_paper = results[0]
        print(f"\n[+] æ­£åœ¨ä¸‹è½½ç¬¬ä¸€ç¯‡è®ºæ–‡: {first_paper['title']}")

        # ä½¿ç”¨arXiv IDä½œä¸ºæ–‡ä»¶å
        filename = f"{first_paper['arxiv_id']}.pdf"
        downloaded_file = searcher.download_pdf(first_paper['pdf_url'], filename=filename)

        if downloaded_file:
            print(f" ä¸‹è½½æˆåŠŸ!")

        else:
            print("[!] ä¸‹è½½å¤±è´¥")





if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œè„šæœ¬æ—¶ï¼Œé»˜è®¤ä¸‹è½½æ·±åº¦å­¦ä¹ è®ºæ–‡
    download_deep_learning_paper()